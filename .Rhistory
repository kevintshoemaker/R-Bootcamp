into = c("Station","climvar")) %>%
pivot_wider(names_from = climvar,
values_from = value)
tidy_clim_data
#  Use dplyr verbs to wrangle data   ----------------------------
# example of simple data selection and summary using group_by, summarize, and mutate verbs
# take tidy_clim_data, then
# group data by station, then
# calculate summaries and put in columns with names mean.precip.in, mean.TMax.F, and mean.Tmin.F, then
# transform to metric and put in new columns mean.precip.in, mean.TMax.F, and mean.Tmin.F
station_mean1 <- tidy_clim_data %>%
group_by(Station) %>%
summarize(
mean.precip.in = mean(PrcpIN, na.rm=TRUE),
mean.TMax.F = mean(TMaxF, na.rm=TRUE),
mean.TMin.F = mean(TMinF, na.rm=TRUE)) %>%
mutate(
mean.precip.mm = mean.precip.in * 25.4,
mean.TMax.C = (mean.TMax.F - 32) * 5 / 9,
mean.TMin.C = (mean.TMin.F - 32) * 5 / 9
)
station_mean1
# using an even more compact form:
# take tidy_clim_data, then
# group data by station, then
# calculate summary (mean of all non-NA values) for numeric data only, then
# transform temp data (.) from F to C, then
# transform precip data (.) from in to mm
station_mean2 <- tidy_clim_data %>%
group_by(Station) %>%
summarize(across(where(is.numeric), mean, na.rm=TRUE)) %>%
mutate(across(c("TMaxF", "TMinF"), ~(.-32)*(5/9)))   %>%
rename_with(~gsub("F","C",.),starts_with("T")) %>%
mutate(across(PrcpIN, ~.*25.4,.names="Prcp_mm")) %>%
select(!PrcpIN)
station_mean2
#  Using lubridate to format and create date data types -----------------
library(lubridate)
date_string <- ("2017-01-31")
# convert date string into date format by identifing the order in which
#   year, month, and day appear in your dates, then arrange "y", "m", and #   "d" in the same order. That gives you the name of the lubridate
#    function that will parse your date
ymd(date_string)
# note the different formats of the date_string and date_dtformat objects in the environment window.
# a variety of other formats/orders can also be accommodated. Note how each of these are reformatted to "2017-01-31" A timezone can be specified using tz=
mdy("January 31st, 2017")
dmy("31-Jan-2017")
ymd(20170131)
ymd(20170131, tz = "UTC")
# can also make a date from components.
#   this is useful if you have columns for year, month,
#   day in a dataframe
year<-2017
month<-1
day<-31
make_date(year, month, day)
# times can be included as well. Note that unless otherwise specified, R assumes UTC time
ymd_hms("2017-01-31 20:11:59")
mdy_hm("01/31/2017 08:01")
# we can also have R tell us the current time or date
now()
today()
#  Parsing dates with lubridate
datetime <- ymd_hms("2016-07-08 12:34:56")
# year
year(datetime)
# month as numeric
month(datetime)
# month as name
month(datetime, label = TRUE)
# day of month
mday(datetime)
# day of year (often incorrectly referred to as julian day)
yday(datetime)
# day of week
wday(datetime)
wday(datetime, label = TRUE, abbr = FALSE)
relig_income
relig_income %>%
pivot_longer(cols=!religion, names_to = "Income", values_to="count")
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, warning = FALSE, message = FALSE)
## 100 samples from a binomial distribution with detection probability p = 0.7.
y <- rbinom(n = n.samples, size = 1, prob = p)
## Use ifelse instead.
detection.history <- ifelse(y == 1, print("Detected"), print("Not detected"))
## Note if...else only works for running one logical (T/F) test at a time. If we have a spreadsheet with lots of data, we need something else.
n.samples <- 100
set.seed(2017)     # the 'seed' allows random number generators to give the same result every time!
## 100 samples from a binomial distribution with detection probability p = 0.7.
y <- rbinom(n = n.samples, size = 1, prob = p)
y
## Use ifelse instead.
detection.history <- ifelse(y == 1, print("Detected"), print("Not detected"))
detection.history
## Note if...else only works for running one logical (T/F) test at a time. If we have a spreadsheet with lots of data, we need something else.
n.samples <- 100
set.seed(2017)     # the 'seed' allows random number generators to give the same result every time!
## 100 samples from a binomial distribution with detection probability p = 0.7.
y <- rbinom(n = n.samples, size = 1, prob = p)
# Draw a sample from a Binomial distribution with p = 0.7 (here, p represents detection probability).
p <- 0.7            # probability of detection
x <- rbinom(n = 1, size = 1, prob = p)      # single 'coin flip' with prob success equal to p
### ifelse() function
## Note if...else only works for running one logical (T/F) test at a time. If we have a spreadsheet with lots of data, we need something else.
n.samples <- 100
set.seed(2017)     # the 'seed' allows random number generators to give the same result every time!
## 100 samples from a binomial distribution with detection probability p = 0.7.
y <- rbinom(n = n.samples, size = 1, prob = p)
y
## Use ifelse instead.
detection.history <- ifelse(y == 1, print("Detected"), print("Not detected"))
detection.history
y
## Use ifelse instead.
detection.history <- ifelse(y == 1, "Detected", "Not detected")
detection.history
## Going the other direction.
ifelse(detection.history == "Detected", 1, 0)
cbind(rbinom(10, 1, .5), rbinom(10, 1, .6))
ifelse(xt[, 1] > 0 & xt[, 2] > 0, "Detected twice",
"Not detected twice")
xt  <-  cbind(rbinom(10, 1, .5), rbinom(10, 1, .6))
xt
ifelse(xt[, 1] > 0 & xt[, 2] > 0, "Detected twice",
"Not detected twice")
## Using the iteration variable "i" within the for loop:
n.iter <- 5
count <- 0
for(i in 1:n.iter){
count <- count+i            # assign a new value of count equal to the old value of count + i
print(count)
}
## A for-loop for dependent sequence (here, the Fibonacci sequence)
n.iter <- 10
x <- rep(0, n.iter)           # set up vector of all zeros
x[1] <- 1                     # assign x_1  <-  1
x[2] <- 1                     # assign x_2 = 0
for(i in 3:n.iter){
x[i] <- x[i-1]+x[i-2]       # x_i = x_(i-1) + x_(i-2)
}
x
n.iter <- 5
count <- 0
for(i in 1:n.iter){
count <- count+i            # assign a new value of count equal to the old value of count + i
print(count)
}
which.max(apply(volcano,1,sd))
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, warning = FALSE, message = FALSE)
# Start with blank workspace -------------------
rm(list=ls())
## We can write our own functions. Useful if we have to repeat the same operations over and over with different inputs.
my.mean <- function(x){       # 'x' is the function argument- 'x' stands in for whatever numeric vector the user wants
m <- sum(x)/length(x)
return(m)
}
foo <- c(2, 4, 6, 8)
my.mean(foo)
## A function to square the arguments.
square <- function(x){
x^2
}
## Square a single value (scalar).
square(2)
## Square all elements of a vector.
square(1:10)
#  if...else statements -----------------------
# Draw a sample from a Binomial distribution with p = 0.7 (here, p represents detection probability).
p <- 0.7            # probability of detection
x <- rbinom(n = 1, size = 1, prob = p)      # single 'coin flip' with prob success equal to p
if (x > 0) {
print("detected")
} else {
print("not detected")
}
#  ifelse()  --------------------------------
## Note if...else only works for running one logical (T/F) test at a time. If we have a spreadsheet with lots of data, we need something else.
n.samples <- 100
## 100 samples from a binomial distribution with detection probability p = 0.7.
y <- rbinom(n = n.samples, size = 1, prob = p)
y
## Use ifelse d
detection.history <- ifelse(y == 1, "Detected", "Not detected")
detection.history
## Going the other direction.
ifelse(detection.history == "Detected", 1, 0)
xt  <-  cbind(rbinom(10, 1, .5), rbinom(10, 1, .6))
xt
ifelse(xt[, 1] > 0 & xt[, 2] > 0, "Detected twice",
"Not detected twice")
rmd2rscript2("module2_1.Rmd")
rmd2rscript2("module2_2.Rmd")
rmd2rscript2("module2_3.Rmd")
rmd2rscript2("module2_4.Rmd")
rmd2rscript <- function(infile="module1_1.Rmd"){    # function for converting markdown to scripts
outfile1 <- gsub(".Rmd",".R",infile)
outfile2 <- gsub(".Rmd",".txt",infile)
close( file( outfile1, open="w" ) )   # clear output file
close( file( outfile2, open="w" ) )   # clear output file
con1 <- file(infile,open="r")
con2 <- file(outfile1,"w")
con3 <- file(outfile2,"w")
stringToFind <- "```{r*"
isrblock <- FALSE
count=0
while(length(input <- readLines(con1, n=1)) > 0){   # while there are still lines to be read
isrblock <- grepl(input, pattern = stringToFind, perl = TRUE)   # is it the start of an R block?
if(isrblock){
while(!grepl(newline<-readLines(con1, n=1),pattern="```",perl=TRUE)){
if(count>1){
write(newline,file=con2,append=TRUE)
write(newline,file=con3,append=TRUE)
}
count=count+1
}
isrblock=FALSE
}
}
closeAllConnections()
}
rmd2rscript2 <- function(infile="LECTURE2.Rmd"){    # function for converting markdown to scripts
outfile <- gsub(".Rmd",".R",infile)
close( file( outfile, open="w" ) )   # clear output file
con1 <- file(infile,open="r")
con2 <- file(outfile,"w")
stringToFind <- "```{r*"
stringToFind2 <- "echo"
isrblock <- FALSE
#count=0
blocknum=0
while(length(input <- readLines(con1, n=1)) > 0){   # while there are still lines to be read
isrblock <- grepl(input, pattern = stringToFind, perl = TRUE)   # is it the start of an R block?
showit <- !grepl(input, pattern = stringToFind2, perl = TRUE)   # is it hidden (echo=FALSE)
if(isrblock){
blocknum=blocknum+1
while(!grepl(newline<-readLines(con1, n=1),pattern="```",perl=TRUE)){
if((blocknum>1)&((showit)|(blocknum==2))) write(newline,file=con2,append=TRUE)
#count=count+1
}
isrblock=FALSE
}
}
closeAllConnections()
}
rmd2rscript2("module1_1.Rmd")
rmd2rscript2("module1_1.Rmd")
rmd2rscript2("module1_2.Rmd")
rmd2rscript2("module1_3.Rmd")
rmd2rscript2("module1_3.Rmd")
rmd2rscript2("module1_4.Rmd")
rmd2rscript2("module2_1.Rmd")
rmd2rscript2("module2_2.Rmd")
rmd2rscript2("module2_3.Rmd")
rmd2rscript2("module2_4.Rmd")
rmd2rscript2("module2_4.Rmd")
rmd2rscript2("module2_5.Rmd")
knitr::opts_chunk$set(
echo = TRUE,
message = FALSE,
warning = FALSE,
cache = TRUE
)
#  R Bootcamp #2, submodule 2.3 -----------------------------
#     University of Nevada, Reno
#     Topic: basic statistics
# STATISTICS! ------------------
#  Load Data ------------------
sculpin.df <- read.csv("sculpineggs.csv")
head(sculpin.df)
# Summary Statistics ------------------
mean(sculpin.df$NUMEGGS)      # compute sample mean
median(sculpin.df$NUMEGGS)    # compute sample median
min(sculpin.df$NUMEGGS)       # sample minimum
max(sculpin.df$NUMEGGS)       # sample maximum
range(sculpin.df$NUMEGGS)     # both min and max.
quantile(sculpin.df$NUMEGGS,0.5)            # compute sample median using quantile function
quantile(sculpin.df$NUMEGGS,c(0.25,0.75))   # compute sample quartiles
var(sculpin.df$NUMEGGS)           # sample variance
sd(sculpin.df$NUMEGGS)            # sample standard deviation
sd(sculpin.df$NUMEGGS)^2          # another way to compute variance
apply(sculpin.df,2,mean)       # column means of data frame
apply(sculpin.df,2,median)     # column medians of data frame
# maybe you'd like to use some tidyverse functions instead:
sculpin.df %>% summarize(across(everything(),mean) )
########
# Or just use the "summary()" function!
summary(sculpin.df) # provides a set of summary statistics for all columns in a data frame.
# Deal with missing data --------------
newdf <- read.table(file="data_missing.txt", sep="\t", header=T)  # load dataset with missing data
mean(newdf$Export)
mean(newdf$Export,na.rm = TRUE)
#  Plot data  (base R)
#
# hist(sculpin.df$NUMEGGS)
# plot(x = sculpin.df$FEMWT,y = sculpin.df$NUMEGGS)
#  Ggplot alternative:
ggplot(sculpin.df,aes(NUMEGGS)) + geom_histogram(bins=5)
ggplot(sculpin.df,aes(FEMWT,NUMEGGS)) + geom_point()
#  Linear Regression   -------------------
m1 <- lm(NUMEGGS ~ FEMWT, data=sculpin.df)      # fit linear regression model
summary(m1)                             # view model summary
summary(m1)$r.squared                   # extract R-squared
confint(m1)                             # confidence intervals for intercept and slope
AIC(m1)                                 # report AIC (Akaike's Information Criterion, used to perform model selection)
plot(x = sculpin.df$FEMWT,y = sculpin.df$NUMEGGS)    # plot data
abline(m1)                                           # plot line of best fit
# Use the "predict()" function! --------------
nd <- data.frame(FEMWT = 30)                   # create new data frame to predict number of eggs at FEMWT of 30
predict(m1,newdata=nd)                         # make prediction
predict(m1,newdata=nd,interval="confidence")   # make prediction and get confidence interval
predict(m1,newdata=nd,interval="prediction")   # make prediction and get prediction interval
#  Model selection example -------------------
m1 <- lm(NUMEGGS ~ FEMWT, data=sculpin.df)                  # fit linear regression model
summary(m1)
m2 <- lm(NUMEGGS ~ 1, data=sculpin.df)                      # fit linear regression with intercept only (null model)
summary(m2)
m3 <- lm(NUMEGGS ~ poly(FEMWT,2), data=sculpin.df)           # fit polynomial regression
summary(m3)
plot(NUMEGGS ~ FEMWT,data=sculpin.df)                      # plot data
abline(m1,col="black")                                     # plot line of best fit
abline(m2,col="red")                                       # plot intercept only model
# Use 'predict' to draw lines on scatterplot  -----------------
#  Here's a flexible method for drawing any arbitrary modeled relationship!
nd <- data.frame(FEMWT = seq(10,45,by=0.1))        # create new data frame to predict number of eggs from FEMWT of 10 to 45 by increments of 0.1
NUMEGGS.pred <- predict(m3,newdata=nd,interval="confidence")             # make prediction using "predict()" function
lines(nd$FEMWT,NUMEGGS.pred[,1],col="green")  # plot sqrt model (note the use of the "lines()" function to draw a line!)
# some of you might want to try recreating this plot using gglot!
# Perform model selection! -------------
#Compare models using AIC
AIC(m1)
AIC(m2)
AIC(m3)
# which model has the lowest AIC?
#  And finally, here's how you can draw a confidence interval or prediction interval around a non-linear regression relationship!
plot(NUMEGGS ~ FEMWT,data=sculpin.df)                      # plot data
NUMEGGS.confint <- predict(m3,newdata=nd,interval="confidence")             # use "predict()" function to compute the confidence interval!
lines(nd$FEMWT,NUMEGGS.confint[,"fit"],col="green",typ="l",lwd=2)  # plot fitted sqrt model
lines(nd$FEMWT,NUMEGGS.confint[,"lwr"],col="green",typ="l",lty=2)  # plot fitted sqrt model
lines(nd$FEMWT,NUMEGGS.confint[,"upr"],col="green",typ="l",lty=2)  # plot fitted sqrt model
# alternative using ggplot:
NUMEGGS.confint2 <- as_tibble(cbind(nd,NUMEGGS.confint))
ggplot() %>% +
geom_point(data=sculpin.df,mapping=aes(FEMWT,NUMEGGS)) +
geom_path(data=NUMEGGS.confint2,aes(x=FEMWT,y=fit)) +
geom_ribbon(data=NUMEGGS.confint2,aes(x=FEMWT,ymin=lwr,ymax=upr),alpha=0.5)
m33 <- lm(NUMEGGS ~ poly(FEMWT,3), data=sculpin.df)
nd <- data.frame(FEMWT = seq(10,45,by=0.1))        # create new data frame to predict number of eggs from FEMWT of 10 to 45 by increments of 0.1
NUMEGGS.confint <- predict(m33,newdata=nd,interval="confidence")             # make prediction using "predict()" function
NUMEGGS.confint2 <- as_tibble(cbind(nd,NUMEGGS.confint))
ggplot() %>% +
geom_point(data=sculpin.df,mapping=aes(FEMWT,NUMEGGS)) +
geom_path(data=NUMEGGS.confint2,aes(x=FEMWT,y=fit)) +
geom_ribbon(data=NUMEGGS.confint2,aes(x=FEMWT,ymin=lwr,ymax=upr),alpha=0.5)
plot(NUMEGGS ~ FEMWT,data=sculpin.df)                      # plot data
NUMEGGS.confint <- predict(m3,newdata=nd,interval="confidence")             # use "predict()" function to compute the confidence interval!
plot(NUMEGGS ~ FEMWT,data=sculpin.df)                      # plot data
NUMEGGS.confint <- predict(m3,newdata=nd,interval="confidence")             # use "predict()" function to compute the confidence interval!
lines(nd$FEMWT,NUMEGGS.confint[,"fit"],col="green",typ="l",lwd=2)  # plot fitted sqrt model
lines(nd$FEMWT,NUMEGGS.confint[,"lwr"],col="green",typ="l",lty=2)  # plot fitted sqrt model
lines(nd$FEMWT,NUMEGGS.confint[,"upr"],col="green",typ="l",lty=2)  # plot fitted sqrt model
# alternative using ggplot:
NUMEGGS.confint2 <- as_tibble(cbind(nd,NUMEGGS.confint))
ggplot() %>% +
geom_point(data=sculpin.df,mapping=aes(FEMWT,NUMEGGS)) +
geom_path(data=NUMEGGS.confint2,aes(x=FEMWT,y=fit)) +
geom_ribbon(data=NUMEGGS.confint2,aes(x=FEMWT,ymin=lwr,ymax=upr),alpha=0.5)
m33 <- lm(NUMEGGS ~ poly(FEMWT,3), data=sculpin.df)
nd <- data.frame(FEMWT = seq(10,45,by=0.1))        # create new data frame to predict number of eggs from FEMWT of 10 to 45 by increments of 0.1
NUMEGGS.confint <- predict(m33,newdata=nd,interval="confidence")             # make prediction using "predict()" function
NUMEGGS.confint2 <- as_tibble(cbind(nd,NUMEGGS.confint))
ggplot() %>% +
geom_point(data=sculpin.df,mapping=aes(FEMWT,NUMEGGS)) +
geom_path(data=NUMEGGS.confint2,aes(x=FEMWT,y=fit)) +
geom_ribbon(data=NUMEGGS.confint2,aes(x=FEMWT,ymin=lwr,ymax=upr),alpha=0.5)
nd <- data.frame(FEMWT=5)
predict(m33,nd,interval="confidence",alpha=0.95)
rmd2rscript2("module1_1.Rmd")
rmd2rscript2("module1_2.Rmd")
rmd2rscript2("module1_2.Rmd")
rmd2rscript2("module1_3.Rmd")
rmd2rscript2("module1_4.Rmd")
rmd2rscript2("module1_4.Rmd")
rmd2rscript2("module2_1.Rmd")
rmd2rscript2("module2_2.Rmd")
rmd2rscript2("module2_3.Rmd")
rmd2rscript2("module2_3.Rmd")
rmd2rscript2("module2_4.Rmd")
rmd2rscript2("module2_5.Rmd")
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
#  R Bootcamp #1, Module 1
#      University of Nevada, Reno
#  Getting started with R: the basics  -----------------------
myname <- "batman"  # or use your real name.
# R DEMO ----------------------
#     don't worry if you don't understand this just yet- this is just a
#     taste of where we are going!
# Install packages
#   NOTE you only have to do this once. If you have not already installed the packages, you can uncomment and run the following lines:
# install.packages(c("ggplot2","tidyverse"))
# Load packages
library(ggplot2)
library(tidyverse)
library(Lahman)    # for getting baseball data
# Read in data (from the web)
salaries <- read_csv("http://dgrtwo.github.io/pages/lahman/Salaries.csv")
master <- read_csv("http://dgrtwo.github.io/pages/lahman/Master.csv")
batting <- read_csv("http://dgrtwo.github.io/pages/lahman/Batting.csv")
# Read in data from package (you can read in all of these from the Lahman package!)
fielding <- tibble(Lahman::Fielding)
# explore the data
salaries
# summary(salaries)    # summary statistics for all variables in data frame
master
# summary(master)
batting
# summary(batting)
fielding
# summary(fielding)
# Do some wrangling!
# merge the batting and salaries data frames
merged.batting = left_join(batting, salaries, by=c("playerID", "yearID", "teamID", "lgID"))
# merge the "master" dataset (player biographical info)
merged.bio = inner_join(merged.batting, master, by="playerID")
# summarize fielding data by year and player- prepare to merge with other data
fielding.temp = fielding %>%
group_by(playerID,yearID,teamID,lgID) %>%     #
summarize(position = first(modeest::mfv(POS)),
games = sum(G))
merged.all = inner_join(merged.bio,fielding.temp,by=c("playerID", "yearID", "teamID", "lgID"))
merged.all = merged.all %>%    # remove all rows with no at-bats...
filter( AB > 0 )
# range(merged.all$AB)
merged.all = merged.all %>%     # make a new column with the full name
mutate(name=paste(nameFirst, nameLast))
# summarize by player
summarized.batters = merged.all %>%
group_by(playerID) %>%
summarise(name=first(name),
League=first(modeest::mfv(lgID)),
Position=first(modeest::mfv(position)),
First.yr=min(yearID),
Total.HR=sum(HR),
Total.R=sum(R),
Total.H=sum(H),
AB=sum(AB),
BattingAverage=sum(H) / sum(AB) ) %>%
arrange(desc(Total.HR))
# visualize the data
# visualize correlation between hits and runs
ggplot(summarized.batters, aes(Total.H, Total.R)) + geom_point()
# visualize histogram of batting average
ggplot(summarized.batters, aes(BattingAverage)) + geom_histogram()
# remove "outliers" and try again
summarized.batters = summarized.batters %>%
filter(AB>100&First.yr>1920)
ggplot(summarized.batters, aes(BattingAverage)) + geom_histogram()
ggplot(summarized.batters, aes(BattingAverage,col=League)) + geom_density()
# Why does NL density plot indicate a sizable number of players with very low batting average?
# make a new variable to indicate whether each player is a pitcher or position player
summarized.batters = summarized.batters %>%
mutate(Pitcher=ifelse(Position=="P","Pitcher","Position Player"))
ggplot(summarized.batters, aes(BattingAverage)) +
geom_histogram(aes(y = ..density..,group=Pitcher)) +
geom_density(aes(col=Pitcher),lwd=1.5) +
stat_function(
fun = dnorm,
args = with(summarized.batters, c(mean = mean(BattingAverage[Position!="P"]),
sd = sd(BattingAverage[Position!="P"]))),
col="lightblue",lwd=1.1
)   +
stat_function(
fun = dnorm,
args = with(summarized.batters, c(mean = mean(BattingAverage[Position=="P"]),
sd = sd(BattingAverage[Position=="P"]))),
col="pink",lwd=1.1
)   +
scale_x_continuous("Batting Average") +
labs(title = "Histogram with Normal Curve")
ggplot(summarized.batters, aes(BattingAverage)) +
geom_histogram(aes(y = ..density..,group=Pitcher)) +
geom_density(aes(col=Pitcher),lwd=1.5) +
stat_function(
fun = dnorm,
args = with(summarized.batters, c(mean = mean(BattingAverage[Position!="P"]),
sd = sd(BattingAverage[Position!="P"]))),
col="lightblue",lwd=1.1
)   +
stat_function(
fun = dnorm,
args = with(summarized.batters, c(mean = mean(BattingAverage[Position=="P"]),
sd = sd(BattingAverage[Position=="P"]))),
col="pink",lwd=1.1
)   +
labs(x="Batting Average",title = "Histogram with Normal Curve")
# Summmarize by time (and league)
summarized.year = merged.all %>%
filter(yearID>1920) %>%
group_by(yearID,lgID) %>%
summarise(Total.HR=sum(HR),
Total.R=sum(R),
Total.H=sum(H),
AB=sum(AB),
BattingAverage=sum(H) / sum(AB) ) %>%
arrange(yearID, lgID)
summarized.year
# visualize the data
# visualize trend in home runs
ggplot(summarized.year, aes(yearID, Total.HR, col=lgID)) +
geom_line()
# visualize trend in batting average
ggplot(summarized.year, aes(yearID, BattingAverage, col=lgID)) +
geom_line()
