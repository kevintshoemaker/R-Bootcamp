## Going the other direction.
ifelse(detection.history == "Detected", 1, 0)
cbind(rbinom(10, 1, .5), rbinom(10, 1, .6))
ifelse(xt[, 1] > 0 & xt[, 2] > 0, "Detected twice",
"Not detected twice")
xt  <-  cbind(rbinom(10, 1, .5), rbinom(10, 1, .6))
xt
ifelse(xt[, 1] > 0 & xt[, 2] > 0, "Detected twice",
"Not detected twice")
## Using the iteration variable "i" within the for loop:
n.iter <- 5
count <- 0
for(i in 1:n.iter){
count <- count+i            # assign a new value of count equal to the old value of count + i
print(count)
}
## A for-loop for dependent sequence (here, the Fibonacci sequence)
n.iter <- 10
x <- rep(0, n.iter)           # set up vector of all zeros
x[1] <- 1                     # assign x_1  <-  1
x[2] <- 1                     # assign x_2 = 0
for(i in 3:n.iter){
x[i] <- x[i-1]+x[i-2]       # x_i = x_(i-1) + x_(i-2)
}
x
n.iter <- 5
count <- 0
for(i in 1:n.iter){
count <- count+i            # assign a new value of count equal to the old value of count + i
print(count)
}
which.max(apply(volcano,1,sd))
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, warning = FALSE, message = FALSE)
# Start with blank workspace -------------------
rm(list=ls())
## We can write our own functions. Useful if we have to repeat the same operations over and over with different inputs.
my.mean <- function(x){       # 'x' is the function argument- 'x' stands in for whatever numeric vector the user wants
m <- sum(x)/length(x)
return(m)
}
foo <- c(2, 4, 6, 8)
my.mean(foo)
## A function to square the arguments.
square <- function(x){
x^2
}
## Square a single value (scalar).
square(2)
## Square all elements of a vector.
square(1:10)
#  if...else statements -----------------------
# Draw a sample from a Binomial distribution with p = 0.7 (here, p represents detection probability).
p <- 0.7            # probability of detection
x <- rbinom(n = 1, size = 1, prob = p)      # single 'coin flip' with prob success equal to p
if (x > 0) {
print("detected")
} else {
print("not detected")
}
#  ifelse()  --------------------------------
## Note if...else only works for running one logical (T/F) test at a time. If we have a spreadsheet with lots of data, we need something else.
n.samples <- 100
## 100 samples from a binomial distribution with detection probability p = 0.7.
y <- rbinom(n = n.samples, size = 1, prob = p)
y
## Use ifelse d
detection.history <- ifelse(y == 1, "Detected", "Not detected")
detection.history
## Going the other direction.
ifelse(detection.history == "Detected", 1, 0)
xt  <-  cbind(rbinom(10, 1, .5), rbinom(10, 1, .6))
xt
ifelse(xt[, 1] > 0 & xt[, 2] > 0, "Detected twice",
"Not detected twice")
rmd2rscript2("module2_1.Rmd")
rmd2rscript2("module2_2.Rmd")
rmd2rscript2("module2_3.Rmd")
rmd2rscript2("module2_4.Rmd")
rmd2rscript <- function(infile="module1_1.Rmd"){    # function for converting markdown to scripts
outfile1 <- gsub(".Rmd",".R",infile)
outfile2 <- gsub(".Rmd",".txt",infile)
close( file( outfile1, open="w" ) )   # clear output file
close( file( outfile2, open="w" ) )   # clear output file
con1 <- file(infile,open="r")
con2 <- file(outfile1,"w")
con3 <- file(outfile2,"w")
stringToFind <- "```{r*"
isrblock <- FALSE
count=0
while(length(input <- readLines(con1, n=1)) > 0){   # while there are still lines to be read
isrblock <- grepl(input, pattern = stringToFind, perl = TRUE)   # is it the start of an R block?
if(isrblock){
while(!grepl(newline<-readLines(con1, n=1),pattern="```",perl=TRUE)){
if(count>1){
write(newline,file=con2,append=TRUE)
write(newline,file=con3,append=TRUE)
}
count=count+1
}
isrblock=FALSE
}
}
closeAllConnections()
}
rmd2rscript2 <- function(infile="LECTURE2.Rmd"){    # function for converting markdown to scripts
outfile <- gsub(".Rmd",".R",infile)
close( file( outfile, open="w" ) )   # clear output file
con1 <- file(infile,open="r")
con2 <- file(outfile,"w")
stringToFind <- "```{r*"
stringToFind2 <- "echo"
isrblock <- FALSE
#count=0
blocknum=0
while(length(input <- readLines(con1, n=1)) > 0){   # while there are still lines to be read
isrblock <- grepl(input, pattern = stringToFind, perl = TRUE)   # is it the start of an R block?
showit <- !grepl(input, pattern = stringToFind2, perl = TRUE)   # is it hidden (echo=FALSE)
if(isrblock){
blocknum=blocknum+1
while(!grepl(newline<-readLines(con1, n=1),pattern="```",perl=TRUE)){
if((blocknum>1)&((showit)|(blocknum==2))) write(newline,file=con2,append=TRUE)
#count=count+1
}
isrblock=FALSE
}
}
closeAllConnections()
}
rmd2rscript2("module1_1.Rmd")
rmd2rscript2("module1_1.Rmd")
rmd2rscript2("module1_2.Rmd")
rmd2rscript2("module1_3.Rmd")
rmd2rscript2("module1_3.Rmd")
rmd2rscript2("module1_4.Rmd")
rmd2rscript2("module2_1.Rmd")
rmd2rscript2("module2_2.Rmd")
rmd2rscript2("module2_3.Rmd")
rmd2rscript2("module2_4.Rmd")
rmd2rscript2("module2_4.Rmd")
rmd2rscript2("module2_5.Rmd")
knitr::opts_chunk$set(
echo = TRUE,
message = FALSE,
warning = FALSE,
cache = TRUE
)
#  R Bootcamp #2, submodule 2.3 -----------------------------
#     University of Nevada, Reno
#     Topic: basic statistics
# STATISTICS! ------------------
#  Load Data ------------------
sculpin.df <- read.csv("sculpineggs.csv")
head(sculpin.df)
# Summary Statistics ------------------
mean(sculpin.df$NUMEGGS)      # compute sample mean
median(sculpin.df$NUMEGGS)    # compute sample median
min(sculpin.df$NUMEGGS)       # sample minimum
max(sculpin.df$NUMEGGS)       # sample maximum
range(sculpin.df$NUMEGGS)     # both min and max.
quantile(sculpin.df$NUMEGGS,0.5)            # compute sample median using quantile function
quantile(sculpin.df$NUMEGGS,c(0.25,0.75))   # compute sample quartiles
var(sculpin.df$NUMEGGS)           # sample variance
sd(sculpin.df$NUMEGGS)            # sample standard deviation
sd(sculpin.df$NUMEGGS)^2          # another way to compute variance
apply(sculpin.df,2,mean)       # column means of data frame
apply(sculpin.df,2,median)     # column medians of data frame
# maybe you'd like to use some tidyverse functions instead:
sculpin.df %>% summarize(across(everything(),mean) )
########
# Or just use the "summary()" function!
summary(sculpin.df) # provides a set of summary statistics for all columns in a data frame.
# Deal with missing data --------------
newdf <- read.table(file="data_missing.txt", sep="\t", header=T)  # load dataset with missing data
mean(newdf$Export)
mean(newdf$Export,na.rm = TRUE)
#  Plot data  (base R)
#
# hist(sculpin.df$NUMEGGS)
# plot(x = sculpin.df$FEMWT,y = sculpin.df$NUMEGGS)
#  Ggplot alternative:
ggplot(sculpin.df,aes(NUMEGGS)) + geom_histogram(bins=5)
ggplot(sculpin.df,aes(FEMWT,NUMEGGS)) + geom_point()
#  Linear Regression   -------------------
m1 <- lm(NUMEGGS ~ FEMWT, data=sculpin.df)      # fit linear regression model
summary(m1)                             # view model summary
summary(m1)$r.squared                   # extract R-squared
confint(m1)                             # confidence intervals for intercept and slope
AIC(m1)                                 # report AIC (Akaike's Information Criterion, used to perform model selection)
plot(x = sculpin.df$FEMWT,y = sculpin.df$NUMEGGS)    # plot data
abline(m1)                                           # plot line of best fit
# Use the "predict()" function! --------------
nd <- data.frame(FEMWT = 30)                   # create new data frame to predict number of eggs at FEMWT of 30
predict(m1,newdata=nd)                         # make prediction
predict(m1,newdata=nd,interval="confidence")   # make prediction and get confidence interval
predict(m1,newdata=nd,interval="prediction")   # make prediction and get prediction interval
#  Model selection example -------------------
m1 <- lm(NUMEGGS ~ FEMWT, data=sculpin.df)                  # fit linear regression model
summary(m1)
m2 <- lm(NUMEGGS ~ 1, data=sculpin.df)                      # fit linear regression with intercept only (null model)
summary(m2)
m3 <- lm(NUMEGGS ~ poly(FEMWT,2), data=sculpin.df)           # fit polynomial regression
summary(m3)
plot(NUMEGGS ~ FEMWT,data=sculpin.df)                      # plot data
abline(m1,col="black")                                     # plot line of best fit
abline(m2,col="red")                                       # plot intercept only model
# Use 'predict' to draw lines on scatterplot  -----------------
#  Here's a flexible method for drawing any arbitrary modeled relationship!
nd <- data.frame(FEMWT = seq(10,45,by=0.1))        # create new data frame to predict number of eggs from FEMWT of 10 to 45 by increments of 0.1
NUMEGGS.pred <- predict(m3,newdata=nd,interval="confidence")             # make prediction using "predict()" function
lines(nd$FEMWT,NUMEGGS.pred[,1],col="green")  # plot sqrt model (note the use of the "lines()" function to draw a line!)
# some of you might want to try recreating this plot using gglot!
# Perform model selection! -------------
#Compare models using AIC
AIC(m1)
AIC(m2)
AIC(m3)
# which model has the lowest AIC?
#  And finally, here's how you can draw a confidence interval or prediction interval around a non-linear regression relationship!
plot(NUMEGGS ~ FEMWT,data=sculpin.df)                      # plot data
NUMEGGS.confint <- predict(m3,newdata=nd,interval="confidence")             # use "predict()" function to compute the confidence interval!
lines(nd$FEMWT,NUMEGGS.confint[,"fit"],col="green",typ="l",lwd=2)  # plot fitted sqrt model
lines(nd$FEMWT,NUMEGGS.confint[,"lwr"],col="green",typ="l",lty=2)  # plot fitted sqrt model
lines(nd$FEMWT,NUMEGGS.confint[,"upr"],col="green",typ="l",lty=2)  # plot fitted sqrt model
# alternative using ggplot:
NUMEGGS.confint2 <- as_tibble(cbind(nd,NUMEGGS.confint))
ggplot() %>% +
geom_point(data=sculpin.df,mapping=aes(FEMWT,NUMEGGS)) +
geom_path(data=NUMEGGS.confint2,aes(x=FEMWT,y=fit)) +
geom_ribbon(data=NUMEGGS.confint2,aes(x=FEMWT,ymin=lwr,ymax=upr),alpha=0.5)
m33 <- lm(NUMEGGS ~ poly(FEMWT,3), data=sculpin.df)
nd <- data.frame(FEMWT = seq(10,45,by=0.1))        # create new data frame to predict number of eggs from FEMWT of 10 to 45 by increments of 0.1
NUMEGGS.confint <- predict(m33,newdata=nd,interval="confidence")             # make prediction using "predict()" function
NUMEGGS.confint2 <- as_tibble(cbind(nd,NUMEGGS.confint))
ggplot() %>% +
geom_point(data=sculpin.df,mapping=aes(FEMWT,NUMEGGS)) +
geom_path(data=NUMEGGS.confint2,aes(x=FEMWT,y=fit)) +
geom_ribbon(data=NUMEGGS.confint2,aes(x=FEMWT,ymin=lwr,ymax=upr),alpha=0.5)
plot(NUMEGGS ~ FEMWT,data=sculpin.df)                      # plot data
NUMEGGS.confint <- predict(m3,newdata=nd,interval="confidence")             # use "predict()" function to compute the confidence interval!
plot(NUMEGGS ~ FEMWT,data=sculpin.df)                      # plot data
NUMEGGS.confint <- predict(m3,newdata=nd,interval="confidence")             # use "predict()" function to compute the confidence interval!
lines(nd$FEMWT,NUMEGGS.confint[,"fit"],col="green",typ="l",lwd=2)  # plot fitted sqrt model
lines(nd$FEMWT,NUMEGGS.confint[,"lwr"],col="green",typ="l",lty=2)  # plot fitted sqrt model
lines(nd$FEMWT,NUMEGGS.confint[,"upr"],col="green",typ="l",lty=2)  # plot fitted sqrt model
# alternative using ggplot:
NUMEGGS.confint2 <- as_tibble(cbind(nd,NUMEGGS.confint))
ggplot() %>% +
geom_point(data=sculpin.df,mapping=aes(FEMWT,NUMEGGS)) +
geom_path(data=NUMEGGS.confint2,aes(x=FEMWT,y=fit)) +
geom_ribbon(data=NUMEGGS.confint2,aes(x=FEMWT,ymin=lwr,ymax=upr),alpha=0.5)
m33 <- lm(NUMEGGS ~ poly(FEMWT,3), data=sculpin.df)
nd <- data.frame(FEMWT = seq(10,45,by=0.1))        # create new data frame to predict number of eggs from FEMWT of 10 to 45 by increments of 0.1
NUMEGGS.confint <- predict(m33,newdata=nd,interval="confidence")             # make prediction using "predict()" function
NUMEGGS.confint2 <- as_tibble(cbind(nd,NUMEGGS.confint))
ggplot() %>% +
geom_point(data=sculpin.df,mapping=aes(FEMWT,NUMEGGS)) +
geom_path(data=NUMEGGS.confint2,aes(x=FEMWT,y=fit)) +
geom_ribbon(data=NUMEGGS.confint2,aes(x=FEMWT,ymin=lwr,ymax=upr),alpha=0.5)
nd <- data.frame(FEMWT=5)
predict(m33,nd,interval="confidence",alpha=0.95)
rmd2rscript2("module1_1.Rmd")
rmd2rscript2("module1_2.Rmd")
rmd2rscript2("module1_2.Rmd")
rmd2rscript2("module1_3.Rmd")
rmd2rscript2("module1_4.Rmd")
rmd2rscript2("module1_4.Rmd")
rmd2rscript2("module2_1.Rmd")
rmd2rscript2("module2_2.Rmd")
rmd2rscript2("module2_3.Rmd")
rmd2rscript2("module2_3.Rmd")
rmd2rscript2("module2_4.Rmd")
rmd2rscript2("module2_5.Rmd")
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
#  R Bootcamp #1, Module 1
#      University of Nevada, Reno
#  Getting started with R: the basics  -----------------------
myname <- "batman"  # or use your real name.
# R DEMO ----------------------
#     don't worry if you don't understand this just yet- this is just a
#     taste of where we are going!
# Install packages
#   NOTE you only have to do this once. If you have not already installed the packages, you can uncomment and run the following lines:
# install.packages(c("ggplot2","tidyverse"))
# Load packages
library(ggplot2)
library(tidyverse)
library(Lahman)    # for getting baseball data
# Read in data (from the web)
salaries <- read_csv("http://dgrtwo.github.io/pages/lahman/Salaries.csv")
master <- read_csv("http://dgrtwo.github.io/pages/lahman/Master.csv")
batting <- read_csv("http://dgrtwo.github.io/pages/lahman/Batting.csv")
# Read in data from package (you can read in all of these from the Lahman package!)
fielding <- tibble(Lahman::Fielding)
# explore the data
salaries
# summary(salaries)    # summary statistics for all variables in data frame
master
# summary(master)
batting
# summary(batting)
fielding
# summary(fielding)
# Do some wrangling!
# merge the batting and salaries data frames
merged.batting = left_join(batting, salaries, by=c("playerID", "yearID", "teamID", "lgID"))
# merge the "master" dataset (player biographical info)
merged.bio = inner_join(merged.batting, master, by="playerID")
# summarize fielding data by year and player- prepare to merge with other data
fielding.temp = fielding %>%
group_by(playerID,yearID,teamID,lgID) %>%     #
summarize(position = first(modeest::mfv(POS)),
games = sum(G))
merged.all = inner_join(merged.bio,fielding.temp,by=c("playerID", "yearID", "teamID", "lgID"))
merged.all = merged.all %>%    # remove all rows with no at-bats...
filter( AB > 0 )
# range(merged.all$AB)
merged.all = merged.all %>%     # make a new column with the full name
mutate(name=paste(nameFirst, nameLast))
# summarize by player
summarized.batters = merged.all %>%
group_by(playerID) %>%
summarise(name=first(name),
League=first(modeest::mfv(lgID)),
Position=first(modeest::mfv(position)),
First.yr=min(yearID),
Total.HR=sum(HR),
Total.R=sum(R),
Total.H=sum(H),
AB=sum(AB),
BattingAverage=sum(H) / sum(AB) ) %>%
arrange(desc(Total.HR))
# visualize the data
# visualize correlation between hits and runs
ggplot(summarized.batters, aes(Total.H, Total.R)) + geom_point()
# visualize histogram of batting average
ggplot(summarized.batters, aes(BattingAverage)) + geom_histogram()
# remove "outliers" and try again
summarized.batters = summarized.batters %>%
filter(AB>100&First.yr>1920)
ggplot(summarized.batters, aes(BattingAverage)) + geom_histogram()
ggplot(summarized.batters, aes(BattingAverage,col=League)) + geom_density()
# Why does NL density plot indicate a sizable number of players with very low batting average?
# make a new variable to indicate whether each player is a pitcher or position player
summarized.batters = summarized.batters %>%
mutate(Pitcher=ifelse(Position=="P","Pitcher","Position Player"))
ggplot(summarized.batters, aes(BattingAverage)) +
geom_histogram(aes(y = ..density..,group=Pitcher)) +
geom_density(aes(col=Pitcher),lwd=1.5) +
stat_function(
fun = dnorm,
args = with(summarized.batters, c(mean = mean(BattingAverage[Position!="P"]),
sd = sd(BattingAverage[Position!="P"]))),
col="lightblue",lwd=1.1
)   +
stat_function(
fun = dnorm,
args = with(summarized.batters, c(mean = mean(BattingAverage[Position=="P"]),
sd = sd(BattingAverage[Position=="P"]))),
col="pink",lwd=1.1
)   +
scale_x_continuous("Batting Average") +
labs(title = "Histogram with Normal Curve")
ggplot(summarized.batters, aes(BattingAverage)) +
geom_histogram(aes(y = ..density..,group=Pitcher)) +
geom_density(aes(col=Pitcher),lwd=1.5) +
stat_function(
fun = dnorm,
args = with(summarized.batters, c(mean = mean(BattingAverage[Position!="P"]),
sd = sd(BattingAverage[Position!="P"]))),
col="lightblue",lwd=1.1
)   +
stat_function(
fun = dnorm,
args = with(summarized.batters, c(mean = mean(BattingAverage[Position=="P"]),
sd = sd(BattingAverage[Position=="P"]))),
col="pink",lwd=1.1
)   +
labs(x="Batting Average",title = "Histogram with Normal Curve")
# Summmarize by time (and league)
summarized.year = merged.all %>%
filter(yearID>1920) %>%
group_by(yearID,lgID) %>%
summarise(Total.HR=sum(HR),
Total.R=sum(R),
Total.H=sum(H),
AB=sum(AB),
BattingAverage=sum(H) / sum(AB) ) %>%
arrange(yearID, lgID)
summarized.year
# visualize the data
# visualize trend in home runs
ggplot(summarized.year, aes(yearID, Total.HR, col=lgID)) +
geom_line()
# visualize trend in batting average
ggplot(summarized.year, aes(yearID, BattingAverage, col=lgID)) +
geom_line()
knitr::opts_chunk$set(
echo = TRUE,
message = FALSE,
warning = FALSE,
cache = TRUE
)
library(tidyverse)
library(ggplot2)
library(zoo)
#  UNR R workshop #1, Module 2
#  Managing data
library(tidyverse)
library(ggplot2)
library(zoo)
# Working directories --------------------
# Find the directory you're working in
getwd()          # note: the results from running this command on my machine will differ from yours!
#  Import/Export data files into R----------------------
# read_csv to import textfile with columns separated by commas
data.df <- read_csv("data.csv")
names(data.df)
# Remove redundant objects from your workspace
rm(data.txt.df)
# Built-in data files
data()
newdf <- data.df[,c("Country","Product")]
write_csv(newdf, file="data_export.csv")   # export a subset of the data we just read in.
saveRDS(b, "Myobject1.rds")    # use saveRDS to save individual R objects
save(a,b,file="Myobjects1.RData")    # use 'save' to save sets of objects
a <- 1
b <- data.df$Product
saveRDS(b, "Myobject1.rds")    # use saveRDS to save individual R objects
save(a,b,file="Myobjects1.RData")    # use 'save' to save sets of objects
save.image("Myworkspace.RData")      # use 'save.image' to save your entire workspace
rm(a,b)   # remove these objects from the environment
new_b <- readRDS("Myobject1.rds")
load("Myworkspace.RData")   # load these objects back in with the same names!
rm(list=ls())   # clear the entire environment. Confirm that your environment is now empty!
data.df <- read_csv("data.csv")  # read the data back in!
a <- 1
b <- data.df$Product
rm(list=ls())   # clear the entire environment. Confirm that your environment is now empty!
data.df <- read_csv("data.csv")  # read the data back in!
a <- 3     # assign the value "3" to the object named "a"
a = 3      # assign the value "3" to the object named "a" (alternative)
a = 5      # assign the value "3" to the object named "a" (alternative)
knitr::opts_chunk$set(
echo = TRUE,
message = FALSE,
warning = FALSE,
cache = TRUE
)
library(tidyverse)
library(ggplot2)
library(zoo)
#  UNR R workshop #1, Module 2
#  Managing data
library(tidyverse)
library(ggplot2)
library(zoo)
# Working directories --------------------
# Find the directory you're working in
getwd()          # note: the results from running this command on my machine will differ from yours!
#  Import/Export data files into R----------------------
# read_csv to import textfile with columns separated by commas
data.df <- read_csv("data.csv")
names(data.df)
# Remove redundant objects from your workspace
rm(data.txt.df)
data(mtcars)   # read built-in data on car road tests performed by Motor Trend
head(mtcars)    # inspect the first few lines
# ?mtcars        # learn more about this built-in data set
ggplot2::diamonds   # note the use of the package name followed by two colons- this is a way to make sure you are using a function (or data set or other object) from a particular package... (sometimes several packages have functions with the same name...)
# Check/explore data objects --------------------------
# ?str: displays the internal structure of the data object
str(mtcars)
str(data.df)
names(diamonds)
summary(data.df)
# Exporting data (save to hard drive as data file)
# ?write_csv: writes a CSV file to the working directory
newdf <- data.df[,c("Country","Product")]
write_csv(newdf, file="data_export.csv")   # export a subset of the data we just read in.
#  Saving and loading
# saveRDS: save a single object from the environment to hard disk
# save: saves one or more objects from the environment to hard disk. Must be read back in with the same name.
a <- 1
b <- data.df$Product
saveRDS(b, "Myobject1.rds")    # use saveRDS to save individual R objects
save(a,b,file="Myobjects1.RData")    # use 'save' to save sets of objects
save.image("Myworkspace.RData")      # use 'save.image' to save your entire workspace
rm(a,b)   # remove these objects from the environment
new_b <- readRDS("Myobject1.rds")
load("Myworkspace.RData")   # load these objects back in with the same names!
# Clear the environment
rm(list=ls())   # clear the entire environment. Confirm that your environment is now empty!
data.df <- read_csv("data.csv")  # read the data back in!
# Working with data in R ------------------------
# <- assignment operator
# =  alternative assignment operator
a <- 3     # assign the value 3 to the object named "a"
b = 5      # assign the value 5 to the object named "b"
a == 3     # answer the question: "does the object "a" equal "3"?
a == b
# what happens if you accidentally typed 'a = b'?
#  Subsetting data
data.df[data.df[,"Import"]<74,]    # select those rows of data for which second column is less than 74
#  or alternatively, using tidyverse syntax (and the pipe operator):
data.df %>%
filter(Import<74)    # use "filter" verb from 'dplyr' package
sub.countries<-c("Chile","Colombia","Mexico")    # create vector of character strings
data.df %>%
filter(Country %in% sub.countries)   # subset the dataset for only that subset of countries we're interested in
#  Practice subsetting a data frame
turtles.df <- read_delim(file="turtle_data.txt",delim="\t")   # tab-delimited file
turtles.df
