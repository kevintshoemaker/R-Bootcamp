<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Parallel processing in R" />


<title>Module 2.4</title>

<script src="site_libs/header-attrs-2.10/header-attrs.js"></script>
<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/journal.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>








<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.tab('show');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">R bootcamp, Fall 2021</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Bootcamp #1: getting started
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="module1_1.html">Submodule 1.1: working with objects</a>
    </li>
    <li>
      <a href="module1_2.html">Submodule 1.2: managing data</a>
    </li>
    <li>
      <a href="module1_3.html">Submodule 1.3: plotting and basic stats</a>
    </li>
    <li>
      <a href="module1_4.html">Submodule 1.4: Programming in R</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Bootcamp #2: Beyond the basics
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="module2_1.html">Submodule 2.1: R packages</a>
    </li>
    <li>
      <a href="module2_2.html">Submodule 2.2: Using the 'tidyverse'</a>
    </li>
    <li>
      <a href="module2_3.html">Submodule 2.3: Plotting with 'ggplot'</a>
    </li>
    <li>
      <a href="module2_4.html">Submodule 2.4: Parallel computing in R</a>
    </li>
    <li>
      <a href="module2_5.html">Submodule 2.5: Spatial analysis</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Data sets
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="data.dat">Example: tab-separated data</a>
    </li>
    <li>
      <a href="data.csv">Example: comma-separated data</a>
    </li>
    <li>
      <a href="data.txt">Example: whitespace-separated data</a>
    </li>
    <li>
      <a href="data_missing.txt">Example: missing data</a>
    </li>
    <li>
      <a href="MTMetStations.csv">Meteorological Data</a>
    </li>
    <li>
      <a href="sculpineggs.csv">Sculpin Data</a>
    </li>
    <li>
      <a href="turtle_data.txt">Turtle Data</a>
    </li>
    <li>
      <a href="geneData.csv">Gene Data</a>
    </li>
    <li>
      <a href="reptiles.csv">Reptile Data</a>
    </li>
    <li>
      <a href="data.zip">GIS Data</a>
    </li>
  </ul>
</li>
<li>
  <a href="Links.html">Links</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Module 2.4</h1>
<h4 class="author">Parallel processing in R</h4>
<h4 class="date">Fall 2021</h4>

</div>


<p>NOTE: Jonathan Greenberg (<a href="https://naes.unr.edu/gears/" class="uri">https://naes.unr.edu/gears/</a>) developed this module</p>
<p>This is the first of the modules that really go <em>beyond the basics</em>. Module 2.5 also provides a “whirlwhind” tour of more advanced things you can do in R, from parallel computing, to sophisticated visualizations to geographic analysis.</p>
<div id="load-script-for-module-2.4" class="section level2">
<h2>Load script for module #2.4</h2>
<ol style="list-style-type: decimal">
<li><p>Click <a href="module2_4.R">here</a> to download the script! Save the script to a convenient folder on your laptop. [alternative: <a href="greenberg_code.R">load Jonathan’s original script</a>]</p></li>
<li><p>Load your script in RStudio. To do this, open RStudio and click on the folder icon in the toolbar.</p></li>
</ol>
<p>Let’s get started with parallel processing in R!</p>
</div>
<div id="running-r-in-parallel" class="section level2">
<h2>Running R in parallel!</h2>
<p>This is a huge topic, but the basic concept of parallel computing is the following:</p>
<p>Given some problem X that can be divided into subproblems x1, x2, x3, each subproblem can be sent to a different “worker” processor (which may be located on a different physical computer).</p>
<p>Each processor then sends the results of its subproblem back to a central “master”. The master often then pieces the subproblems back together to return to the user.</p>
<p>Parallel computing is not a cure-all, and not all problems will benefit from it. We’ll come back to this issue in a bit.</p>
<p>There are a lot of packages to realize parallel computing within R, and current versions of R even come with some parallel computing packages built-in. A full list can be found <a href="http://cran.r-project.org/web/views/HighPerformanceComputing.html">here</a>.</p>
<p>I will be following the examples from <a href="http://trg.apbionet.org/euasiagrid/docs/parallelR.notes.pdf">http://trg.apbionet.org/euasiagrid/docs/parallelR.notes.pdf</a>.</p>
<div id="set-up-your-workspace" class="section level3">
<h3>Set up your workspace</h3>
<p>First, let’s set up our workspace to run an example using a genomics data set. This example uses <a href="https://www.bioconductor.org/">bioconductor</a>, which provides tools for the analysis and comprehension of high-throughput genomic data. Bioconductor uses R and is open source and open development.</p>
<p>Installing the packages for this module can take a while, so hopefully you have already had a chance to do this prior to coming to this workshop!</p>
<pre class="r"><code>################
# Set up the workspace (can take a surprisingly long time!)

# examples from http://trg.apbionet.org/euasiagrid/docs/parallelR.notes.pdf

# First, load this script directly from the web (which also installs required packages for you!):

if (!requireNamespace(&quot;BiocManager&quot;, quietly = TRUE))
    install.packages(&quot;BiocManager&quot;)
BiocManager::install(&quot;Biobase&quot;)</code></pre>
<p>NOTE: if you had trouble running the above code, you can just load the example dataset <a href="geneData.csv">here</a></p>
<p>We are going to run a test using gene expression data– let’s load the example dataset!</p>
<pre class="r"><code>data(geneData, package = &quot;Biobase&quot;)   # load example data from Biobase package between all pairs of genes across all samples.
  # geneData &lt;- read.csv(&quot;geneData.csv&quot;)    # alternative!


# This data represents 500 genes (organized by rows)
# with expression data (numerical). 

# The goal is to calculate the correlation coefficient
# between all pairs of genes across all samples.  For example,
# to test gene 12 vs 13:</code></pre>
</div>
<div id="example-a-simple-yet-computationally-intensive-problem" class="section level3">
<h3>Example: A simple yet computationally intensive problem</h3>
<p>This data represents 500 genes (organized by rows) with expression data (numerical) across 26 samples. The goal is to calculate the correlation coefficient between all pairs of genes across all samples.</p>
<p>For example, let’s examine the correlation between gene 12 vs 13:</p>
<pre class="r"><code>?cor    # view help page for &quot;cor()&quot; function
geneData[12,]
geneData[13,]</code></pre>
<pre class="r"><code>plot(geneData[12,],geneData[13,])    # plot the correlation between two genes</code></pre>
<p><img src="module2_4_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<pre class="r"><code>cor(geneData[12,],geneData[13,])</code></pre>
<pre><code>## [1] 0.548477</code></pre>
<p>The total number of paired correlations will be 500 x 499 / 2 = <strong>124,750 correlations</strong>!</p>
<p>We can identify all combinations in a vector using the "combn() function:</p>
<pre class="r"><code>?combn    # function to determine all combination ids</code></pre>
<p>We are going to leave this as a list, for now…</p>
<pre class="r"><code>pair &lt;- combn(1:nrow(geneData),2,simplify=F)   #leave this as a list, for now...

length(pair)</code></pre>
<pre><code>## [1] 124750</code></pre>
<pre class="r"><code>head(pair,n=3)     # note that the &quot;head()&quot; and &quot;tail()&quot; functions can be used on lists in addition to data frames...</code></pre>
<pre><code>## [[1]]
## [1] 1 2
## 
## [[2]]
## [1] 1 3
## 
## [[3]]
## [1] 1 4</code></pre>
<pre class="r"><code>tail(pair,n=3)</code></pre>
<pre><code>## [[1]]
## [1] 498 499
## 
## [[2]]
## [1] 498 500
## 
## [[3]]
## [1] 499 500</code></pre>
<p>It often makes sense to break a problem like this down a set of repeated tasks, each of which is expressed as a function.</p>
<p>Let’s write (and test) a function to accept a pair in the database and returns the correlation:</p>
<pre class="r"><code>############
# New function: accepts a gene pair and the database as arguments and returns
# the correlation:

geneCor &lt;- function(pair,gene=geneData){
  cor(gene[pair[1],],gene[pair[2],])
}

# Test the function:
cor(geneData[12,],geneData[13,])</code></pre>
<pre><code>## [1] 0.548477</code></pre>
<pre class="r"><code>geneCor(pair=c(12,13),gene=geneData)    # should be the same result as the previous command...</code></pre>
<pre><code>## [1] 0.548477</code></pre>
<p>We left our pairs as a list, so we can use <em>lapply</em>! Recall that <em>lapply</em> takes each element in a list (in sequence), applies a specified function to that element, and returns the results from the function in another list.</p>
<p>Let’s just test the first 3 pairs:</p>
<pre class="r"><code>########
# use &quot;lapply&quot; to run our function on the first three gene pairs

pair[1:3]</code></pre>
<pre><code>## [[1]]
## [1] 1 2
## 
## [[2]]
## [1] 1 3
## 
## [[3]]
## [1] 1 4</code></pre>
<pre class="r"><code>outcor &lt;- lapply(pair[1:3],geneCor)
outcor</code></pre>
<pre><code>## [[1]]
## [1] 0.1536158
## 
## [[2]]
## [1] 0.7066034
## 
## [[3]]
## [1] -0.2125437</code></pre>
<p>Now let’s do the whole calculation. First, open your task manager by hitting control-alt-delete (on Windows) and choosing “Start Task Manager”.</p>
<p>Click the “Performance” tab. You may also need to click on another link to open the “Resource Monitor”. Notice you have a few graphs next to “CPU Usage”, which shows each processor you have on your machine.</p>
<p>Type the following in R and watch the CPU Usage History graph. Note the use of the “system.time()” function, which allows us to keep track of processing time for one or more R commands. Processing time can vary substantially among machines.</p>
<pre class="r"><code>#################
# Examine processor use and speed

# First, open your task manager to view processor usage (see website for more details).

system.time(outcor &lt;- lapply(pair,geneCor))</code></pre>
<pre><code>##    user  system elapsed 
##    3.61    0.00    3.61</code></pre>
<pre class="r"><code># Notice ONE CPU spiked.  Make note of how long it took to run.</code></pre>
<p>Notice ONE CPU spiked. Make note of how long it took to run.</p>
<p>Now let’s make an even bigger dataset (just repeating geneData four times), making 26 x 4 = 104 columns:</p>
<pre class="r"><code>fakeData &lt;- cbind(geneData,geneData,geneData,geneData)    # make an even bigger dataset!</code></pre>
<p>Now lets make a more complex, computationally intensive function that calculates the 95% confidence intervals of the correlation coefficients through a process of <em>bootstrapping</em>:</p>
<pre class="r"><code>###########
# New, more complex function that generates bootstrap confidence intervals for correlation coefficients

library(boot)   # load the &quot;boot&quot; library for performing bootstrapping analysis

# &#39;x&#39; represents vector with two elements, indicating the pair of genes to compare
# &#39;gene&#39; represents a gene expression database

geneCor2 &lt;- function(x, gene = fakeData){
  mydata &lt;- cbind(gene[x[1], ], gene[x[2], ])      # extract the rows (genes) to compare
  mycor &lt;- function(x, i) cor(x[i,1], x[i,2])     # function (correlation) to perform bootstrap analysis with
  boot.out &lt;- boot(mydata, mycor, 1000)           # perform bootstrap analysis (1000 iterations)
  boot.ci(boot.out, type = &quot;bca&quot;)$bca[4:5]         # extract and return the bootstrap confidence interval
}</code></pre>
<p>How long would it take to run correlation analyses using this new function for just 10 gene pairs?</p>
<pre class="r"><code># Test how long 10 pairs would take:
genCor2_system_time &lt;- system.time(outcor &lt;- lapply(pair[1:10],geneCor2))
genCor2_system_time[&quot;elapsed&quot;] </code></pre>
<pre><code>## elapsed 
##    0.82</code></pre>
<p>What did you get? On my machine this operation (for 10 pairs) took around 1 second…</p>
<p>Let’s use this information to estimate how long it will take to run this for the all pairs! How many pairs were there again?</p>
<pre class="r"><code>length(pair)   # how many pairs were there again?</code></pre>
<pre><code>## [1] 124750</code></pre>
<p>Okay, so there are 124,750 pairs… So let’s figure out how long 124,750 pairs will take:</p>
<pre class="r"><code>############
# estimate time to run 124,750 pairs

genCor2_system_time[&quot;elapsed&quot;]      # time it took to run 10 pairs, in seconds</code></pre>
<pre><code>## elapsed 
##    0.82</code></pre>
<pre class="r"><code>genCor2_system_time[&quot;elapsed&quot;] *(124750/10)     # time it would take to run 124750 pairs, in seconds </code></pre>
<pre><code>## elapsed 
## 10229.5</code></pre>
<pre class="r"><code>genCor2_system_time[&quot;elapsed&quot;] *(124750/10/60/60)    # in hours...</code></pre>
<pre><code>##  elapsed 
## 2.841528</code></pre>
<p>Ouch! This is a good candidate for parallel processing!</p>
</div>
<div id="running-in-parallel" class="section level3">
<h3>Running in Parallel!</h3>
<p>Let’s take a subsample of all the pairs for exploring this further:</p>
<pre class="r"><code>###########
# Explore parallel processing vs sequential processing

pair2 &lt;- sample(pair,300)    # first extract a subsamble of all the pairs</code></pre>
<p>The opposite of parallel processing is “sequential processing”. Let’s first test a sequential version of this task:</p>
<pre class="r"><code># Let&#39;s test a sequential version of this:
system.time(outcor &lt;- lapply(pair2,geneCor2))</code></pre>
<pre><code>##    user  system elapsed 
##   21.95    0.35   22.30</code></pre>
<pre class="r"><code># Note the elapsed time.</code></pre>
<p>Now, we can explore implementing this operation in parallel. To do this, we will use the “parallel” package in R.</p>
<div id="parallel-package" class="section level4">
<h4>‘parallel’ package</h4>
<p>Parallel’s basic concept is to launch “worker” instances of R on other processors, and then send the subproblems to each R instance. This means that if you want to use 4 processors (“cores”), after you start a parallel cluster, you will have 5 copies of R running: the master copy (the one you are typing into) and 4 worker copies.</p>
<p>The “cluster” of processors can be either a single computer with multiple processors (like the one you are working on), or a network of computers linked by some clustering framework.</p>
<p>The basic order of operations with using ‘parallel’ is as follows: 1. library(“parallel”)<br />
2. Make a parallel cluster using makeCluster(…)<br />
3. Load packages that your function needs into the workers using clusterEvalQ(cl=…,library(…))<br />
4. Load objects (data) from the master environment into the worker environments using clusterExport(cl=…)<br />
5. Following basic lapply() semantics, use e.g. clusterApplyLB(cl=…) to apply your function to an input list, where each iteration of the “loop” will be sent to an available processor. The output is usually a list.<br />
6. Shut down your cluster using stopCluster(…)</p>
<p>So the first step is to load the ‘parallel’ package:</p>
<pre class="r"><code>###########
# Run this operation in parallel!!

### parallel: built-in parallel computation package.
library(&quot;parallel&quot;)       # load the package (comes with basic R installation)</code></pre>
<p>We’re also going to grab the “future” package for later use:</p>
<pre class="r"><code>install.packages(&quot;future&quot;)    # install &quot;future&quot; package, which we will use later</code></pre>
<pre class="r"><code>library(&quot;future&quot;)</code></pre>
<p>The way parallel works, is we first have to make an R cluster via “makeCluster()”:</p>
<pre class="r"><code>?makeCluster    # learn more about &quot;makeCluster()&quot;</code></pre>
<p>We are going to create a cluster with 4 cpus of type “PSOCK”.</p>
<p>Click the task manager “Processes” (Windows) or Activity Monitor (mac), and scroll down to the “R”s:</p>
<pre class="r"><code>########
# Make a cluster

myCluster &lt;- parallel::makeCluster(spec=4,type=&quot;PSOCK&quot;)    # make cluster with 4 cpus of type &quot;PSOCK&quot;
myCluster</code></pre>
<pre><code>## socket cluster with 4 nodes on host &#39;localhost&#39;</code></pre>
<pre class="r"><code>length(myCluster) # One entry per &quot;worker&quot;.</code></pre>
<pre><code>## [1] 4</code></pre>
<pre class="r"><code>myCluster[[1]]    # more info about this &quot;worker&quot;</code></pre>
<pre><code>## node of a socket cluster on host &#39;localhost&#39; with pid 3172</code></pre>
<p>Notice you now have multiple instances of Rscript.exe running now!</p>
<p>We can stop the cluster by using the “stopCluster()” function:</p>
<pre class="r"><code>?stopCluster</code></pre>
<pre class="r"><code>parallel::stopCluster(myCluster)</code></pre>
<p>Now let’s make a new cluster using all available cores.</p>
<pre class="r"><code>######
# Make a cluster with all available cores

mycorenum &lt;- availableCores()   # determine number of available cores
myCluster &lt;- parallel::makeCluster(spec=mycorenum,type=&quot;PSOCK&quot;)</code></pre>
<p>We can send the same function to each node (worker) in the cluster using “clusterCall()”:</p>
<pre class="r"><code>######
# Run a function on each cluster:

?clusterCall       # clustercall(): sends the same function to each node (worker) in the cluster</code></pre>
<p>For example, we might run the “date()” function on each cluster! Note that the results get returned as lists:</p>
<pre class="r"><code>date()    # run the &quot;date()&quot; function</code></pre>
<pre><code>## [1] &quot;Fri Sep 17 14:55:12 2021&quot;</code></pre>
<pre class="r"><code>workerDates &lt;- parallel::clusterCall(cl=myCluster,fun=date)    # now try running the &quot;date()&quot; function on all clusters 
class(workerDates)</code></pre>
<pre><code>## [1] &quot;list&quot;</code></pre>
<pre class="r"><code>length(workerDates) # One list element per worker.</code></pre>
<pre><code>## [1] 4</code></pre>
<p>You need to remember that each worker instance of R that is running is essentially “empty” – it won’t, be default, have access to the environment of the master. Thus, we need to send some commands/data to them in anticipation of running the code.</p>
<p>For example, the “search()” function tells us what packages and data are loaded (attached). Let’s explore what packages are loaded in our worker environments:</p>
<pre class="r"><code>####
# explore packages loaded in our worker environments

search()      # packages loaded in global environment
workerPackages &lt;- parallel::clusterCall(cl=myCluster,fun=search)     # .. and worker environments
# workerPackages</code></pre>
<p>We are using a package called “boot” in our function, so we need to load up this package on every worker using the “clusterEvalQ()” function:</p>
<pre class="r"><code>#########
# load required packages on every worker:

?clusterEvalQ</code></pre>
<pre class="r"><code>loadOnCls &lt;- parallel::clusterEvalQ(cl=myCluster,library(&quot;boot&quot;))   </code></pre>
<pre class="r"><code># We can confirm the boot package is now loaded:
parallel::clusterCall(cl=myCluster,fun=search)</code></pre>
<p>Next, we will export the dataset “fakeData” to each worker using the “clusterExport()” function.</p>
<p>First, look at the worker Rs and note how much memory they are using (in task manager).</p>
<pre class="r"><code>########
# Load required data on every worker

?clusterExport</code></pre>
<pre class="r"><code>loadData &lt;- parallel::clusterExport(cl=myCluster,&quot;fakeData&quot;)</code></pre>
<pre class="r"><code>parallel::clusterEvalQ(cl=myCluster,ls())   # Check the environment on each worker- make sure data are loaded</code></pre>
<p>Check the memory now. Note that each worker is using more memory, since we loaded the fakeData into each of those worker Rs.</p>
<p>Note we could send the entire Global environment over to the workers! To do this, we could use the “ls()” function:</p>
<pre class="r"><code>#### alternatively, load all data in our environment into each worker:
# loadData &lt;- parallel::clusterExport(cl=myCluster,ls())</code></pre>
<pre class="r"><code># parallel::clusterEvalQ(cl=myCluster,ls())</code></pre>
</div>
<div id="finally-run-corrlation-analysis-in-parallel" class="section level4">
<h4>Finally: run corrlation analysis in parallel!</h4>
<p>Now comes the actual function call. To do this, we use the “clusterApplyLB()” function:</p>
<pre class="r"><code>############
# Spread function calls across workers using an &quot;lapply&quot;-like function

?clusterApplyLB     # function for performing the actual function call</code></pre>
<p>This is VERY similar to an ‘lapply’ statement, except we identify the cluster to send the command to. Let’s call the “geneCor2()” function for each gene pair, using parallel processing. We can compare the speed of the parallel vs sequential processing using the “system.time()” function.</p>
<p>Let’s call the “geneCor2()” function for each gene pair, using parallel processing!</p>
<p>Watch your Performance tab as your cores light up:</p>
<pre class="r"><code>#######
# Call the &quot;geneCor2()&quot; function for each gene pair, using parallel processing

system.time(outcor2 &lt;- parallel::clusterApplyLB(cl=myCluster,pair2,geneCor2))   </code></pre>
<pre><code>##    user  system elapsed 
##    0.14    0.02    6.09</code></pre>
<pre class="r"><code># vs the non-clustered version:
system.time(outcor &lt;- lapply(pair2,geneCor2))   # takes much longer!</code></pre>
<pre><code>##    user  system elapsed 
##   22.19    0.08   22.26</code></pre>
<p>We got a big performance boost here!</p>
<p>Don’t forget to shut down your cluster:</p>
<pre class="r"><code>stopCluster(myCluster)   # finally, stop the cluster..</code></pre>
</div>
</div>
<div id="foreach-making-parallel-computing-even-easier" class="section level3">
<h3>‘foreach’: making parallel computing EVEN EASIER</h3>
<p>There are multiple parallel “backends” to R, including ‘parallel’, ‘snow’, ‘multicore’, ‘Rmpi’, to name a few.</p>
<p>‘foreach’ is a meta-wrapper that works on many parallel backends. What this means is you can write one set of code, and not have to modify it if the user prefers to use Rmpi instead of parallel (in which case the commands are very different).</p>
<p>The basic order of ops with using ‘foreach’ is as follows:</p>
<ol style="list-style-type: decimal">
<li>library(“foreach”)<br />
</li>
<li>Load a parallel backend and foreach registration package<br />
e.g. library(“doParallel”)<br />
</li>
<li>Start a parallel backend with e.g. makeCluster(…)<br />
</li>
<li>Register the parallel backend with foreach,<br />
e.g.: registerDoParallel(…)<br />
</li>
<li>Use foreach(…) %dopar% function() to run your function in parallel.<br />
</li>
<li>Use ‘.packages’ parameter in foreach(…) to load needed packages.<br />
</li>
<li>Stop your cluster using e.g. stopCluster(…)<br />
</li>
<li>Register the sequential backend to foreach using registerDoSEQ()</li>
</ol>
<pre class="r"><code>##########
# Parallel processing using &#39;foreach&#39;

install.packages(&quot;foreach&quot;)
# Install the parallel backend to foreach:
install.packages(&quot;doParallel&quot;)</code></pre>
<pre class="r"><code>library(&quot;foreach&quot;)</code></pre>
<pre><code>## Warning: package &#39;foreach&#39; was built under R version 4.1.1</code></pre>
<pre class="r"><code>library(&quot;doParallel&quot;)</code></pre>
<pre><code>## Warning: package &#39;doParallel&#39; was built under R version 4.1.1</code></pre>
<pre><code>## Loading required package: iterators</code></pre>
<pre><code>## Warning: package &#39;iterators&#39; was built under R version 4.1.1</code></pre>
<pre class="r"><code>?foreach</code></pre>
<p>Try running in sequential mode…</p>
<pre class="r"><code># sequential mode:
registerDoSEQ() # Avoids warnings

system.time(
  outcor &lt;- foreach(
    p = pair2, 
    .packages=&quot;boot&quot;, 
    .combine=&quot;rbind&quot;) %dopar% { 
      return(geneCor2(p)) 
    }
)</code></pre>
<pre><code>##    user  system elapsed 
##   22.48    0.11   22.60</code></pre>
<p>And now with a parallel backend!</p>
<pre class="r"><code># Parallel mode (use a parallel backend):

# Create a cluster using parallel:
myCluster &lt;- makeCluster(spec=4,type=&quot;PSOCK&quot;)</code></pre>
<pre class="r"><code># Register the backend with foreach (doParallel):
registerDoParallel(myCluster)
# Run our code! Notice I didn&#39;t change the foreach call at all:

system.time(
  outcor &lt;- foreach(
    p = pair2, 
    .packages=&quot;boot&quot;,
    .combine=&quot;rbind&quot;) %dopar% {
      return(geneCor2(p)) 
    }
)</code></pre>
<pre><code>##    user  system elapsed 
##    0.09    0.01    6.07</code></pre>
<p>Some things to notice:<br />
- we explicitly define what packages are to be sent over using the .packages parameter<br />
- foreach calls have access to the master’s global environment. - we can use specific functions to join the data once its done (e.g. rbind)</p>
<p>Don’t forget to stop the cluster:</p>
<pre class="r"><code>stopCluster(myCluster)     # Don&#39;t forget to stop the cluster:</code></pre>
<p>You should also register the default, non-parallel backend for ‘foreach’. Otherwise the next time you use ‘foreach’ it will not work.</p>
<pre class="r"><code>registerDoSEQ()    # register the default, non-parallel backend for foreach</code></pre>
</div>
<div id="sources-of-overhead" class="section level3">
<h3>Sources of overhead</h3>
<p>Parallel computing, optimally, should be linear in terms of number of processors vs. time. However, this is never the case. A process running on one core does not take twice the time as a process running on two cores. There are losses along the way from various sources. These need to be thought about when writing the most efficient code.</p>
<p>First, many programs will have parallel and non-parallel components. If your non-parallel components take X amount of time, no matter how many processors you have available, you will never run faster than X amount of time.</p>
<div id="chunking" class="section level4">
<h4>Chunking</h4>
<p>Chunking is one of the most important considerations. So far, we’ve been iterating one “row” at a time. For faster computations, this may not be very efficient, as the overhead of sending/receiving/managing the parallel cluster swamps out gains from the processing. We can get more clever with this by sending MULTIPLE rows at one time to a worker. This optimization of the chunk size (number of rows to send at one time to a single worker) can dramatically speed up your computation, at the cost of heavier RAM usage.</p>
</div>
<div id="shared-memory-machines" class="section level4">
<h4>Shared-memory machines</h4>
<p>A single computer like the one you are working on is a “shared memory machine” – each processor has access to the same physical RAM. When two processors “ask” for an R object in RAM they must compete against each other. One process will have to wait until the other process are done reading that area of memory. Memory access, thus, can be a bottleneck.</p>
<p>####Networked systems of computers A cluster computer (“supercomputer”) is really just a bunch of individual computers networked together. Each individual computer (“node” in cluster terminology) has its own set of processors, memory, and hard drive space. When the master R process does things like sends variables to the workers, it sends this data through a network, which is a lot slower than sending it within a single computer. This transfer of data causes relatively severe bottlenecks that can be ameliorated by faster networking or more clever code.</p>
<p>For instance, you could copy the data the function will work on to each node in a cluster, so the data does not have to be transferred from the master R to the worker Rs.</p>
<p>Note that two nodes in a cluster do not share memory.</p>
</div>
<div id="other-things-to-worry-about" class="section level4">
<h4>Other things to worry about</h4>
<p><strong>RACE CONDITIONS.</strong> This comes in to play if your workers are all writing to the same output file (say, a raster file). As we discussed before, in general only one “thread” (worker) can write to a file at a time, otherwise the file may get corrupt. In parallel computing, we can increase the chance that this might happen if we aren’t careful. This often requires a programmer to be able to “lock” a file – i.e. before a worker tries to write to a file, it 1) checks if the file is “locked”, 2) if it finds it unlocked it first locks the file itself, 3) it opens/writes/close to the file, 4) it unlocks the file for other workers.</p>
<p><strong>Embarrassingly parallel applications and those that aren’t:</strong> the term “embarrassingly parallel” refers to problems that are absolutely trivial to parallelize. The function we’ve been looking at is embarassingly parallel. Each iteration takes (basically) the same amount of time, uses the same amount of memory, doesn’t require cross-talk between workers, etc. Raster processing, as we will see, is often an embarrassingly parallel problem – we process an image one line at a time, sending each line to a different processor.</p>
<p>Vector based processing, on the other hand, is often not embarrassingly parallel, and takes a lot more thought to properly parallelize a function.</p>
<p>Parallel computing takes a lot of practice and understanding of the underlying systems to get good at. Other issues start popping up such as load balancing (what if the functions take non-constant amounts of time on each processor), optimization of the chunk size, and a host of other issues.</p>
<p>Debugging can also be tricky, as the various parallel backends do not typically allows workers to print to your main screen, so you have to have them dump their outputs to a file that you then do a post-mortem on. ‘foreach’ makes this a bit easier, since you can test a function on a single node (using registerDoSEQ()), debug it, and once it works on one node, try it out on a cluster.</p>
<p><a href="module2_5.html">–go to next module–</a></p>
</div>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
